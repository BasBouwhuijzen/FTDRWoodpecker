---
title: "Assignment 2"
author: Judith van der Wolf (4661672) Jesse Nieuwkoop (1689959) Bas Bouwhuijzen (2130616)
  Jay de Jager (6990703)
date: "16-01-2025"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      expand: 3
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: '4'
subtitle: Fundamental Techniques in Data Science with R
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Introduction

For the second assignment we chose a new dataset that will hopefully give us better regression results. We are using a dataset called Customer Behavior from Kaggle. The data represents details about 400 clients of a company including the unique ID, the gender, the age of the customer, the salary, and the information regarding the buying decision - whether the customer decided to buy specific products or not. This dataset contains data on a new unknown product as part of a market assessment. We have a target variable `purchased` with values 0 or 1 (respectively no, and yes) indicating if the product was purchased by the customer. The other variables are `gender`, `age`, and `estimated_salary`. We will use these to predict whether the product was purchased. The `gender` variable has two values in our dataset: male and female. The `age` variable is a continuous numeric value. The `estimated_salary` is a value filled in by the customer with increments of 1000.

## Research Question

“What variables are the best predictors to whether the product was purchased or not?”

# Preperation
 
## Libraries

```{r}
library(tidyverse)    # For data manipulation
library(ggplot2)      # For plotting
library(ggpubr)       # For ggplot grid arrange
library(kableExtra)   # For fancy tables

library(lmtest)       # Assumption Checks
library(regclass)     # Assumption Checks
library(caret)        # Assumption Checks
```

## Read data

```{r}
# Read data
data <- read.csv("Customer_Behaviour.csv")

# View data structure
dplyr::glimpse(data)
```

# Preprocessing

## Rename

```{r}
# Rename column names to snake_case format
data <- data %>%
  dplyr::rename(
    user_id = User.ID,
    gender = Gender,
    age = Age,
    estimated_salary = EstimatedSalary,
    purchased = Purchased
  )
```

## Duplicates

The data was checked for duplicates in user_id, because each used_id number should be unique in the dataset. Zero duplicates were found, thus no recards were removed.

```{r}
# Show count of duplicates
sum(duplicated(data$user_id))
```

## Missing values

The data was checked for missing values. No missing values were present.

```{r}
# Show count of NA values
data %>%
  dplyr::summarise(across(everything(), ~ sum(is.na(.)))) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "NA Count"
  )
```

## Convert to factor

The categorical variables (`gender` and `purchased`) were converted to factor data types.

```{r}
# Convert variables to factor types
data <- data %>%
  dplyr::mutate(
    gender = as.factor(gender),
    purchased = factor(purchased, levels = c(0, 1), labels = c("No", "Yes"))
  )
```

```{r}
# View new structure
str(data$gender)
str(data$purchased)
```

```{r}
# Show category distribution
table(data$gender)
table(data$purchased)
```

## Variable inspection

### Functions

#### capitalize_str()

A function was created to reformat object names to fancy string formatting when creating tables or plots.

```{r}
# Function to format column name to spaced, capitalized string
capitalize_str <- function(string) {
  # Replace underscores with spaces and capitalize each word
  formatted_name <- gsub("_", " ", string)
  formatted_name <- tools::toTitleCase(formatted_name)
  
  # Return result
  return(formatted_name)
}
```

#### summary_table_cat()

A function was created to efficiently display the value distribution of categorical variables.

```{r}
# Function to display comparison summary table
summary_table_cat <- function(data1, data2 = NULL, variable, sort = "none") {
  # Summarize data1
  summary_data1 <- data1 %>%
    group_by(across(all_of(variable))) %>%
    summarise(
      `Frequency (Before)` = n(),
      `Percentage (Before)` = (n() / nrow(data1)) * 100
    )
  
  if (!is.null(data2)) {
    # Summarize data2 if provided
    summary_data2 <- data2 %>%
      group_by(across(all_of(variable))) %>%
      summarise(
        `Frequency (After)` = n(),
        `Percentage (After)` = (n() / nrow(data2)) * 100
      )
    
    # Merge summaries
    result <- full_join(summary_data1, summary_data2, by = variable) %>%
      mutate(
        `Percentage (Before)` = sprintf("%.2f%%", `Percentage (Before)`),
        `Percentage (After)` = sprintf("%.2f%%", `Percentage (After)`)
      )
    
    # Apply sorting
    if (sort == "asc") {
      result <- result %>% arrange(`Frequency (Before)`)
    } else if (sort == "desc") {
      result <- result %>% arrange(desc(`Frequency (Before)`))
    }
  } else {
    # For single dataset case
    result <- summary_data1 %>%
      mutate(
        `Percentage (Before)` = sprintf("%.2f%%", `Percentage (Before)`)
      ) %>%
      rename(`Frequency` = `Frequency (Before)`,
             `Percentage` = `Percentage (Before)`)
    
    # Apply sorting
    if (sort == "asc") {
      result <- result %>% arrange(Frequency)
    } else if (sort == "desc") {
      result <- result %>% arrange(desc(Frequency))
    }
  }
  
  # Rename the variable column
  result <- result %>%
    rename(!!capitalize_str(variable) := all_of(variable))
  
  # Display table
  result <- result %>%
    kbl() %>%
    kable_classic() %>%
    kable_styling(latex_options = c("striped"), full_width = F)
  
  # Return table
  return(result)
}

```

#### summary_table_nums()

A function was created to efficiently display the summary descriptives of a dataset and its given continuous variables.

```{r}
# Create function to create summary table for numerical variables
summary_table_nums <- function(data, variables) {
  # Compile summary table
  summary <- data.frame(
    Mean = sapply(variables, function(var) round(mean(data[[var]]), 3)),
    Min = sapply(variables, function(var) min(data[[var]])),
    Median = sapply(variables, function(var) median(data[[var]])),
    Max = sapply(variables, function(var) max(data[[var]])),
    SD = sapply(variables, function(var) round(sd(data[[var]]), 3))
  )
  
  # Temporarily rename rownames using capitalize_str
  rownames(summary) <- sapply(variables, capitalize_str)
  
  # Display summary table
  summary <- summary %>%
    kbl() %>%
    kable_classic(latex_options = c("striped"), full_width = TRUE)
  
  # Return table
  return(summary)
}
```

#### plot_histogram()

A function was created to easily plot the distribution of a given continuous variable in a histogram.

```{r}
# Function to create a histogram
plot_histogram <- function(data, variable, title = NULL, binwidth = NULL, bins = NULL) {
  
  # Compile plot
  plot <- ggplot(data, aes(x = .data[[variable]])) +
    geom_histogram(binwidth = binwidth, 
                   bins = bins, 
                   fill = "lightblue",
                   color = "black") +
    labs(
      title = title,
      x = capitalize_str(variable),
      y = "Frequency",
      fill = variable
    ) +
    theme_bw()
  
  # Return plot
  return(plot)
}
```

### Categorical variables

#### 1. gender

```{r}
# Verify structure of factors
str(data$gender)

# Verify levels of factors
levels(data$gender)
```

```{r}
# Show summary table
(v1_table <- summary_table_cat(data, 
                               variable = "gender")
)
```

##### Table of value frequency for gender

```{r}
# Show bar plot
(v1_plot <- ggplot(data) +
  geom_bar(aes(x = gender, fill = gender)) +
  scale_fill_manual(values = c("Male" = "lightblue", 
                               "Female" = "lightpink")) +
  labs(title = "Bar Plot of Gender Distribution",
       fill = "Gender",
       x = "Gender",
       y = "Frequency"
      ) +
  theme_bw()
)
```

#### 2. purchased

```{r}
# Verify structure of factors
str(data$purchased)

# Verify levels of factors
levels(data$purchased)
```

```{r}
# Show summary table
(v2_table <- summary_table_cat(data, 
                               variable = "purchased")
)
```

##### Table of value frequency for purchased

```{r}
# Show bar plot
(v2_plot <- ggplot(data) +
  geom_bar(aes(x = purchased, fill = purchased)) +
  scale_fill_manual(values = c("No" = "tomato", 
                               "Yes" = "lightgreen")) +
  labs(title = "Bar Plot of Purchased Distribution",
       fill = "Purchased",
       x = "Purchased",
       y = "Frequency"
      ) +
  theme_bw()
)
```

### Continuous variables

#### Table of summary descriptives for continuous variables

```{r}
summary_table_nums(data, c("age", 
                               "estimated_salary")
                   )
```

#### 3. age

```{r}
# Plot histogram
(v3_plot <- plot_histogram(data, 
                           "age", 
                           "Distribution of Age",
                            binwidth = 1)
)
```

#### 4. estimated_salary

`estimated_salary` is divided by 1000 for interpretation purposes.

```{r}
data <- data %>%
  mutate(
    estimated_salary = estimated_salary / 1000
  )
```

```{r}
# Plot histogram
(v4_plot <- plot_histogram(data, 
                           "estimated_salary", 
                           "Distribution of Estimated Salary",
                            binwidth = 3)
)
```

### General description

After plotting all the predictor variables there were no unrealistic or unnatural values discovered. Furthermore, there were also no (theoretical) outliers present in the dataset. Since the dataset we are working with was flagged as 'clean', this was expected. This means that the dataset did not have to be modified in any way to be able to continue performing logistic regression.

# Logistic Regression

## Functions

### compare_models()

A function was created to efficiently compare fit measures across logistic (or linear) models.

```{r}
# Function to compare fit measures of multiple models
compare_models <- function(data, response_var, compare_with_first = FALSE, model_type = "lm", ...) {
  # Capture models and their names
  models <- list(...)
  model_names <- as.character(substitute(list(...)))[-1] # Extract model names
  num_models <- length(models)
  
  # Initialize variables to store results
  aic_vals <- numeric(num_models)
  bic_vals <- numeric(num_models)
  mse_vals <- if (model_type == "lm") numeric(num_models) else NULL
  r_squared <- if (model_type == "lm") numeric(num_models) else NULL
  deviance_rss <- c(NA, rep(NA, num_models - 1))
  deviance_pvalue <- c(NA, rep(NA, num_models - 1))
  
  # Loop through models to compute fit measures
  for (i in seq_along(models)) {
    aic_vals[i] <- AIC(models[[i]])
    bic_vals[i] <- BIC(models[[i]])
    
    if (model_type == "lm") {
      mse_vals[i] <- mean(models[[i]]$residuals^2)
      r_squared[i] <- summary(models[[i]])$r.squared
    }
    
    # Perform ANOVA for model comparison
    if (i > 1) {
      if (compare_with_first) {
        base_model <- models[[1]]
      } else {
        base_model <- models[[i - 1]]
      }
      anova_test <- anova(base_model, models[[i]], test = ifelse(model_type == "glm", "Chisq", NULL))
      deviance_rss[i] <- if (model_type == "glm") anova_test$Deviance[2] else anova_test$RSS[2]
      deviance_pvalue[i] <- anova_test$`Pr(>Chi)`[2]
    }
  }
  
  # Compile results into a tibble
  fit_measures <- tibble(
    Model = capitalize_str(model_names),
    AIC = aic_vals,
    BIC = bic_vals
  )
  
  # Add R^2 and MSE for linear regression models
  if (model_type == "lm") {
    fit_measures <- fit_measures %>%
      mutate(
        MSE = mse_vals,
        `R^2` = round(r_squared, 3)
      )
  }
  
  # Add deviance for both model types
  fit_measures <- fit_measures %>%
  mutate(
    `Dev. RSS` = deviance_rss,
    `Dev. p-value` = deviance_pvalue
  )
  
  # Fancy kable formatting
  fit_measures <- fit_measures %>%
    kbl() %>%
    kable_classic(latex_options = c("striped"), full_width = TRUE)
  
  # Return fit measures
  return(fit_measures)
}
```

## Models

*Note that for models 1_1, 1_2, and 1_3, the fit measures are compared to the null model. These fit measure comparisons can be found right after the interpretation of model 1_3 and before the interpretation of model 2.*

*However, the fit measures of model 2 are compared to the fit measures of model 1_2, and the fit measures of model 3 are compared to the fit measures of model 2. These fit measure comparisons can be found right after the interpretation of model 3.*

### Null Model

The log-odds for whether a product is purchased is -0.586 (intercept), and the odds are 0.556.

```{r}
# Define the null model
model_null <- glm(purchased ~ 1,
                  data = data,
                  family = binomial)

# Model summary and log-odds
summary(model_null)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_null))
```

### Model 1.1: gender

The reference category is "Female," with a log-odds of `purchased` of **-0.500** and corresponding odds of **0.606**. For men, the log-odds are **0.178** lower than for women, with odds of **0.837** (approximately **17% lower than women**). However, this difference is not statistically significant ($p = 0.396$), suggesting that `gender` is not a meaningful predictor of product purchase in this dataset.

The null deviance (**521.57**) and slightly lower residual deviance (**520.85**) indicate minimal improvement in fit, and the higher AIC (**524.85**) and BIC (**532.83**) further suggest that the null model is a better fit overall. The residual deviance for the gender model (**0.722**) is slightly lower than the null deviance, but this difference is not significant ($p = 0.395$), further supporting that adding gender does not meaningfully improve the model's fit. Thus, adding `gender` as a predictor does not meaningfully enhance the model's performance.

```{r}
# Logistic regression with Gender as the sole predictor
model_1.1 <- glm(purchased ~ 
                 gender, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_1.1)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_1.1))
```

### Model 1.2: age

The intercept (**-8.044**) represents the log-odds of product purchase for individuals aged 0, with corresponding odds close to 0 (**0.00032**), which is effectively negligible. For every additional year of age, the log-odds of product purchase increase by **0.189**, indicating that older individuals are more likely to purchase the product. The odds of purchase increase by  **1.208** for each additional year, meaning a 20.8% increase in the likelihood of purchase per year of age. This relationship is statistically significant ($p < 0.01$), suggesting that age is a strong predictor of product purchase.

The model fit comparison shows that adding age as a predictor (Model 1.2) significantly improves the model fit compared to the null model. Model 1.2 has a much lower AIC (**340.26**) and BIC (**348.24**) compared to the null model's AIC (**523.57**) and BIC (**527.56**). The residual deviance decreases substantially by **185.31**, and the difference is statistically significant ($p < 0.01$). These results indicate that age is a meaningful predictor, providing a significant improvement in model performance.

```{r}
# Logistic regression with Gender as the sole predictor
model_1.2 <- glm(purchased ~ 
                 age, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_1.2)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_1.2))
```

### Model 1.3: estimated_salary

The intercept (**-2.323**) represents the log-odds of product purchase for individuals with an estimated salary of 0, with corresponding odds of approximately **0.098**, indicating that the likelihood of purchase is very low at this salary level. For every one unit increase in estimated salary (1000 dollars), the log-odds of product purchase increase by **0.024**, suggesting that higher salaries are associated with a higher likelihood of product purchase. The odds of purchase increase by a factor of **1.024** for each additional unit, meaning a **2.4% increase in the likelihood of purchase per 1000 dollars increase of salary**, which is statistically significant ($p < 0.01$).

The model fit comparison shows that adding estimated salary as a predictor (Model 1.3) significantly improves the model fit compared to the null model. Model 1.3 has a lower AIC (**471.73**) and BIC (**479.71**) compared to the null model's AIC (**523.57**) and BIC (**527.56**). The residual deviance decreases by **53.85**, and the difference is statistically significant ($p < 0.01$). These results indicate that estimated salary is a meaningful predictor of product purchase, improving the model’s performance.

```{r}
# Logistic regression with Gender as the sole predictor
model_1.3 <- glm(purchased ~ 
                 estimated_salary, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_1.3)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_1.3))
```

##### Table of fit measure comparison (Model 1.1, 1.2, 1.3 & Null Model)

In the table below, model 1.1, 1.2, and 1.3 are compared to the fit of the null model.

```{r}
# Comparing model to null model
compare_models(data,
              "purchased",
              TRUE,
              "glm",
              model_null,
              model_1.1,
              model_1.2,
              model_1.3)
```

### Model 2: age + estimated_salary

The intercept (**-12.43**) represents the log-odds of product purchase when both age and estimated salary are held constant at 0, with corresponding odds of approximately **0**, indicating that the likelihood of purchase is extremely low at this baseline. When age increases by one year, the log-odds of product purchase increase by **0.234**, and the odds increase by a factor of **1.263**, meaning a **26.3% increase in the likelihood of purchase per year of age** while controlling for estimated salary. This relationship is statistically significant ($p < 0.01$).

When estimated salary increases by one unit, the log-odds of product purchase increase by **0.036**, and the odds increase by **1.037**, meaning a **3.7% increase in the likelihood of purchase per unit of salary** while controlling for age. This relationship is also statistically significant ($p < 0.01$).

The model fit comparison shows that adding both age and estimated salary as predictors (Model 2) significantly improves the model fit compared to Model 1.2 (which only includes age). Model 2 has a lower AIC (**283.05**) and BIC (**295.03**) compared to Model 1.2’s AIC (**340.26**) and BIC (**348.24**). The residual deviance decreases by **59.21**, and the difference is statistically significant ($p < 0.01$). These results indicate that the inclusion of estimated salary as a predictor further improves the model’s performance, suggesting that both age and estimated salary together are meaningful predictors of product purchase.

```{r}
# Logistic regression with Gender as the sole predictor
model_2 <- glm(purchased ~ 
                 age +
                 estimated_salary, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_2)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_2))
```

### Model 3: age * estimated_salary

Adding an interaction term between age and estimated salary reveals a significant interaction effect ($p < 0.01$), indicating that the relationship between estimated salary and the odds of product purchase depends on age, and vice versa. This suggests that the effect of estimated salary on product purchase is not constant across all ages and that the effect of age on product purchase varies with estimated salary. The odds of the interaction effect are close to 1, meaning that for every one-unit increase in both estimated salary and age, the odds of product purchase slightly decreases by approximately **0.00061%**. This slight decrease may be attributed to differences in the relationship between age, estimated salary, and product purchase behavior. For instance, younger individuals with higher incomes might spend more on products compared to older individuals with similar earnings. 

The interaction term could not be visualized due to the continuous measurement level of age and the very small decrease, however the plots below do visualize the probability of product purchase vs age and the probability of product purchase vs estimated salary. 

The model fit comparison shows that adding the interaction term (Model 3) significantly improves the model fit compared to Model 2 (which includes only main effects). Model 3 has a much lower AIC (**221.57**) and BIC (**237.54**) compared to Model 2’s AIC (**283.05**) and BIC (**295.03**). The residual deviance decreases by **63.48** compared to Model 2, and the difference is statistically significant ($p < 0.01$). This reduction in deviance indicates that the interaction term captures additional variability in product purchase behavior that is not explained by the main effects alone. These results highlight that the interaction between `age` and `estimated_salary` is a meaningful predictor, further improving the model's explanatory power. 

```{r}
model_3 <- glm(purchased ~ 
                 age * estimated_salary, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_3)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_3))
```



```{r}
data <- data %>%
  mutate(probability = predict(model_3, type = "response"))

# Show plot       
ggplot(data, aes(estimated_salary, probability)) +
  geom_point() +
  geom_jitter() +
  labs(x = "estimated_salary",
       y = "probability",
       title = "Probability of Product Purchase vs estimated_salary") +
  theme_bw()

# Show plot
ggplot(data, aes(age, probability)) +
  geom_point() +
  geom_jitter() +
  labs(x = "age",
       y = "probability",
       title = "Probability of Product Purchase vs age") +
  theme_bw()
```

##### Table of fit measure comparison (Model 1.2, 2 & 3)

In the table below, each model is compared to the fit measures of the model before. So model 2 is compared to model 1.2, and model 3 is compared to model 2.

```{r}
# Comparing model to null model
compare_models(data,
              "purchased",
              FALSE,
              "glm",
              model_1.2,
              model_2,
              model_3)
```

### Final Model

Model 3 will be selected as the final model because it demonstrates the best fit, with the lowest AIC and BIC among the compared models. Additionally, the significant reduction in residual deviance compared to Model 2 indicates that the interaction term meaningfully improves the model's explanatory power. This makes Model 3 the most appropriate choice for capturing the relationship between age, estimated salary, and product purchase.

## Assumptions

### A1: Linearity

```{r}
data$logit <- predict(model_3, type = "link")
```

As can be seen, the linearity of age to log odds is relatively  constant, Which means that age has a relatively consistent influence on the log odds of purchasing.

```{r}
data %>% 
ggplot(aes(age, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess",
              formula = 'y ~ x') + 
  labs(title = "Linearity of Age to Log-odds",
       x = "Age",
       y = "Logit") +
  theme_bw()
```

As can be seen, the linearity of estimated_salary to log odds is relatively constant, Which means that salary has a relatively consistent influence on the log odds of purchasing.

```{r}
data %>% 
ggplot(aes(estimated_salary, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(formula = 'y ~ x',
              method = "loess") + 
  labs(title = "Linearity of Estimated Salary to Log-odds",
     x = "Estimated Salary",
     y = "Logit") +
  theme_bw()
```

### A2: Predictor Matrix Rank

The VIF values in this model are quite high (exceeding the threshold of 10), indicating potential multicollinearity among the predictors. However, the high VIF values are expected because the model includes an interaction term (age * estimated_salary). Interaction terms are mathematically derived from the main effects, creating strong correlations and naturally leading to inflated VIF values. While this can make interpretation of the main effects challenging, it does not necessarily invalidate the model. For the sake of this assignment, it was decided to continue with this model (Model 3).

```{r}
# Retrieve VIF values
vif_values <- car::vif(model_3, type = 'predictor')

# Force the output into a GVIF-like structure
gvif_table <- tibble(
  Variable = capitalize_str(names(vif_values)),
  VIF = vif_values,
  df = 1,
  GVIF = (vif_values^(1 / (1 + df)))
)

# Fancy kable table formatting
gvif_table %>%
  kbl() %>%
  kable_classic(latex_options = c("striped"), full_width = TRUE)
```

### A3: IID Binomial

#### Binary Outcome

The outcome is independently and identically binomially distributed. The bar plot shows there are two outcomes for purchased, so the outcome meets the outcome of a binomial distribution. 257 participants did not buy the product and 143 did. 

```{r}
data %>% 
  ggplot(aes(x = purchased, fill = purchased)) +
  geom_bar() +
  labs(x = "purchased",
       y = "Count",
       title = "Distribution of the outcome variable") +
  theme_bw()
```

```{r}
levels(data$purchased)
table(data$purchased)
```

#### Clustering

We didn't expect residual clustering, because the only variables in the dataset are whether or not a product is purchased, gender and people's estimated salary. There is no reason to believe that there were other variables that could have influenced the variation among the groups, at least no other variables that were measured. 

### Sufficient Sample Size

The minimum sample size is 112. The actual sample size is 400. This means that the assumption for sufficient sample size is met.

```{r}
# Show sample size
(sample_size <- nrow(data))
```

```{r}
# Calculate proportion of purchased
(purchased <- data %>%
  count(purchased) %>%
  mutate(prop = n / sum(n)))
```
```{r}
# Return smallest proportion of negative or positive cases.
(p <- min(purchased$prop))
```

```{r}
# Return number of predictors (k) of final model
(k <- length(coef(model_3)))
```

```{r}
# Calculate the minimum number of cases (n)
(n <- ceiling((10 * k) / p))
```

### Balanced Outcomes

The assumption of balanced outcomes was met.

```{r}
# Show distribution of outcomes
with(data, table(purchased) / length(purchased))
```

### Perfect Prediction

The model does not contain perfectly separable classes, therefore the assumption of perfect prediction is met.

```{r}
predicted_probs <- predict(model_3, type = "response")

perfect_predictions <- predicted_probs == 0 | predicted_probs == 1
any(perfect_predictions) 
```

### Influential Cases

As can be seen, there are no points that fall outside of the the dashed lines (Cook's distance), hence there are no influential outliers that would have to be treated.

```{r}
plot(model_3, which = c(4,5))
```

# Predicted Probabilities

```{r}
data <- data %>%
  mutate(piHat = predict(model_3, type = "response"),
    yHat = as.factor(ifelse(piHat <= 0.5, "No", "Yes"))
  )
```

## Confusion Matrix

```{r}
(conf <- confusionMatrix(data = data$yHat, reference = data$purchased))
conf$byClass["F1"]
```

## Overall Performance

- **Accuracy:** 0.8875 (88.75%)
  This means that 88.75% of the predictions made by the model are correct. Such a high accuracy indicates strong overall performance. However, since the "No" class dominates (64.25%), accuracy alone does not fully capture the model's ability to handle the less frequent "Yes" class.

- **95% Confidence Interval (CI):** (0.8524, 0.9167)
  The CI indicates that the true accuracy is likely to fall between 85.24% and 91.67% in repeated samples. The narrow range reflects a reliable and precise estimate of model performance.

- **No Information Rate (NIR):** 0.6425 ($p < 0.01)
  The NIR represents the accuracy achievable by always predicting the majority class ("No"). Since the model's accuracy (88.75%) is significantly higher than the NIR, it demonstrates strong performance beyond random or naive predictions. The extremely small p-value indicates that the improvement in accuracy over the NIR is statistically significant and unlikely to have occurred by chance.

- **Kappa:** 0.7555
  Kappa measures agreement between predictions and actual outcomes, accounting for chance agreement. A value of 0.7555 indicates substantial agreement beyond chance, signifying consistent and reliable predictions.

## Class-Specific Performance

- **Sensitivity (Recall) for 'No':** 0.9105 (91.05%)
  Sensitivity measures the model's ability to correctly identify "No" cases. A high sensitivity of 91.05% means the model effectively captures the majority of the "No" class, minimizing false negatives for this class.

- **Specificity for 'Yes':** 0.8462 (84.62%)
  Specificity indicates the model's ability to correctly identify "Yes" cases. While slightly lower than sensitivity, the strong specificity (84.62%) shows the model's capacity to detect the "Yes" class while minimizing false positives.

- **Positive Predictive Value (Precision) for 'No':** 0.9141 (91.41%)
  Precision reflects the proportion of predicted "No" cases that are actually "No." The high precision means the model has a low rate of false positives for the "No" class.

- **Negative Predictive Value for 'Yes':** 0.8403 (84.03%)
  This metric represents the proportion of predicted "Yes" cases that are truly "Yes." Though slightly lower than precision, it remains at an acceptable level, indicating reliable predictions for the "Yes" class.

- **Balanced Accuracy:** 0.8783 (87.83%)
  Balanced accuracy is the average of sensitivity and specificity, providing a comprehensive view of performance across both classes. A score of 87.83% confirms the model's robustness in handling both "No" and "Yes" predictions effectively.

- **F1 Score:** 0.9123 (91.23%)
  The F1 score balances precision and recall, offering a single metric to evaluate performance on the positive class ("No"). The high F1 score of 91.23% demonstrates the model's ability to maintain accuracy in identifying true positives while minimizing both false positives and false negatives.

---

## Conclusion

The predictors of product purchase decisions were analyzed using logistic regression models, with `gender`, `age`, and `estimated_salary` serving as independent variables. The analysis revealed that age and estimated_salary are statistically significant predictors of purchasing behavior, both showing a positive correlation with the likelihood of making a purchase. `gender` demonstrated an insignificant effect. Model comparisons, based on fit measures (AIC and BIC), identified the interaction model (`age x estimated_salary`) as the most explanatory. The model yields high accuracy in predictability, high sensitivity and F1 score. Which suggests excellent performance in identifying positive cases (product bought). However, specificity is lower, meaning “not purchased” predictions are less reliable.

All relevant statistical assumptions were tested and satisfied, supporting the validity of the final model. To address the research question, “What variables are the best predictors of whether the product was purchased or not?” the findings highlight that `age` and `estimated_salary` are the strongest predictors of purchase behavior, with their interaction providing valuable insights into their combined influence on customer decisions.
    