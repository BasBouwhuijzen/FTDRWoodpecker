---
title: "Assignment 2"
subtitle: "Fundamental Techniques in Data Science with R"
author: "Judith van der Wolf (4661672) Jesse Nieuwkoop (1689959) Bas Bouwhuijzen (2130616) Jay de Jager (6990703)"
date: "16-12-2024"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      expand: 3
    df_print: paged
---
#Libraries
```{r}
library(dplyr)
```

```{r}
data <- read.csv("Customer_Behaviour.csv")
glimpse(data)
```

#Rename 
```{r}
# Rename column names to snake_case format
data <- data %>%
  dplyr::rename(
    User_ID = User.ID
  )
```

#change to factor 
```{r}
data <- data %>%
  dplyr::mutate(
    Gender = as.factor(Gender),
    Purchased  = as.factor(Purchased )
  )
```

#summary NA
```{r}
sum(is.na(data))
```

#Inspect duplicates
```{r}
sum(duplicated(data$User_ID))
```

#Individual variable check 
```{r}
#counts
table(data$Gender)
table(data$Purchased)
```

-Dataset beschrijven; variabelen, onderzoeksvraag
-Datacleaning 
-Variable check individueel -> indien rare shit, verwijderen
-Grafiekjes maken Boxplots (categorisch) + histograms (continuous) (Jesse :) ) 
-Model bepalen (
      -empty beginnen
      -empty model + inkomen
      -empty model + inkomen + age
      -empty model + inkomen + age + gender
      )
      
-Model comparisons - fitmeasures vergelijken
#Null model
```{r null-model}
# Define the null model
model_null <- glm(Purchased ~ 1, data = data, family = binomial)
summary(model_null)
```
Null model intercept of -0.5862 converts to 35.75% purchasing probability. Found by converting the exponential of the log-odds to statistical odds.
As it is the null model, highly significant and strong prediction with outcome.

#Model 1: Gender as predictor
```{r model-1}
# Logistic regression with Gender as the sole predictor
model_1 <- glm(Purchased ~ Gender, data = data, family = binomial)
summary(model_1)
```
Here, Gender: 0 is assumed as "Female", the reference category. The intercept for that category is -0.5004. 37.50% purchasing probability

For GenderMale, the "Male" category, the coefficent is -0.1775. Being male reduces the log-odds of purchasing by 0.1775. Lower probability of purchasing compared to being female. The P-value is 0.395858, which is not statistically significant. Therefore Gender does not significantly affect the purchasing probability in this model.

Model Fit:

Null deviance is 512.7, the same as the null model. Residual deviance is slightly lower, but minimal.

The AIC is 1 point higher than the null model's AIC, which suggests it is a worse model in terms of fit + complexity. The slightly lower residual deviance does not outweigh the penalty for adding the extra parameter.

#Model 2: Gender + Age
```{r model-2}
# Logistic regression with Gender and Age as predictors
model_2 <- glm(Purchased ~ Gender + Age, data = data, family = binomial)
summary(model_2)
```
Intercept of -8.11537, low probability of purchase, when age is 0. The z and p values determine the intercept to be highly statistically significant.

GenderMale indicates that being male increases the log-odds of purchasing by 0.09468 compared to females, holding Age constant. However, this effect is not statistically significant.

The coefficient for Age indicates that each one-unit increase in age increases the log-odds of purchasing by 0.18954, holding Gender constant. Older individuals are much more likely to purchase. For each additional year of age, the odds increase approximately by e^0.18954 â‰ˆ1.21 (a 21% increase in odds per year of age). Highly significant.

Much lower residual deviance (336.14) indicates better fit. Suggests age greatly improves model fit.

The AIC is much lower than both the null model (523.57) and Model 1 (524.85).

The addition of Age significantly improves the model's explanatory power while justifying the slight increase in complexity by 1 parameter.

#Model 3: Gender + Age + Estimated Salary
```{r model-3}
# Logistic regression with all predictors
model_3 <- glm(Purchased ~ Gender + Age + EstimatedSalary, data = data, family = binomial)
summary(model_3)
```
Intercept is at -12.78, when all predictors are at baseline (Gender is Female, Age and EstimatedSalary are both 0). Highly statistically significant, according to the p and z values.

The coefficient for GenderMale suggests that being male increases the log-odds of purchase by 0.3338, while Age and EstimatedSalary remain constant. However, it is a small effect and not statistically significant (p-value: 0.274).

Each increase of age by 1 unit increases the log-odds of purchase by 0.2370, holding Gender and Estimated Salary constant. This corresponds to an increase of 27% in odds for each additional year of age. Age is highly statistically significant.

The coefficient for EstimatedSalary suggests that for every unit increase in salary, the log-odds of purchase increase by 0.00003644, while Gender and Age remain constant. This is statistically significant. This effect seems small, but do not forget that it applies to every single unit of currency.

The residual deviance has dropped significantly compared to all other models. Indicating that EstimatedSalary provides great improvement in model fit.

#Comparing Model Fit

```{r model-comparison}
# Compare models using AIC
model_comparison <- tibble(
  Model = c("Null Model", "Model 1", "Model 2", "Model 3"),
  AIC = c(AIC(model_null), AIC(model_1), AIC(model_2), AIC(model_3))
)
model_comparison <- model_comparison %>%
  arrange(AIC)  # Sort models by AIC

# Display Model Comparison Table
print(model_comparison)
```
Model 3 is the best fit, with a significantly lower AIC value. The addition of Age and EstimatedSalary significantly improves the model fit.

-Assumptions 
  
  
-Conclusie 
