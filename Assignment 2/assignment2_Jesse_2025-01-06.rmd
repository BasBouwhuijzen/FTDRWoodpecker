---
title: "Assignment 2"
subtitle: "Fundamental Techniques in Data Science with R"
author: "Judith van der Wolf (4661672) Jesse Nieuwkoop (1689959) Bas Bouwhuijzen (2130616) Jay de Jager (6990703)"
date: "06-01-2025"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      expand: 3
    df_print: paged
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Introduction

@

## Research Question

@

# Preperation
 
## Libraries

```{r}
library(tidyverse)    # For data manipulation
library(ggplot2)      # For plotting
library(ggpubr)       # For ggplot grid arrange
library(kableExtra)   # For fancy tables

library(lmtest)       # Assumption Checks
library(regclass)     # Assumption Checks
```

## Read data

```{r}
# Read data
data <- read.csv("Customer_Behaviour.csv")

# View data structure
dplyr::glimpse(data)
```

# Preprocessing

## Rename

```{r}
# Rename column names to snake_case format
data <- data %>%
  dplyr::rename(
    user_id = User.ID,
    gender = Gender,
    age = Age,
    estimated_salary = EstimatedSalary,
    purchased = Purchased
  )
```

## Duplicates

The data was checked for duplicates in user_id, because each used_id number should be unique in the dataset. Zero duplicates were found, thus no recards were removed.

```{r}
# Show count of duplicates
sum(duplicated(data$user_id))
```

## Missing values

```{r}
# Show count of NA values
data %>%
  dplyr::summarise(across(everything(), ~ sum(is.na(.)))) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "NA Count"
  )
```

## Convert to factor

```{r}
# Convert variables to factor types
data <- data %>%
  dplyr::mutate(
    gender = as.factor(gender),
    purchased = factor(purchased, levels = c(0, 1), labels = c("No", "Yes"))
  )
```

```{r}
# View new structure
str(data$gender)
str(data$purchased)
```

```{r}
# Show category distribution
table(data$gender)
table(data$purchased)
```

## Individual variable inspection

```{r}
# Store data before outlier removal for later comparison
data_pre_outlier_removal <- data
```

### Functions

#### capitalize_str()

A function was created to reformat object names to fancy string formatting when creating tables or plots.

```{r}
# Function to format column name to spaced, capitalized string
capitalize_str <- function(string) {
  # Replace underscores with spaces and capitalize each word
  formatted_name <- gsub("_", " ", string)
  formatted_name <- tools::toTitleCase(formatted_name)
  
  # Return result
  return(formatted_name)
}
```

#### summary_table_cat()

A function was created to efficiently display the value distribution of categorical variables.

```{r}
# Function to display comparison summary table
summary_table_cat <- function(data1, data2 = NULL, variable, sort = "none") {
  # Summarize data1
  summary_data1 <- data1 %>%
    group_by(across(all_of(variable))) %>%
    summarise(
      `Frequency (Before)` = n(),
      `Percentage (Before)` = (n() / nrow(data1)) * 100
    )
  
  if (!is.null(data2)) {
    # Summarize data2 if provided
    summary_data2 <- data2 %>%
      group_by(across(all_of(variable))) %>%
      summarise(
        `Frequency (After)` = n(),
        `Percentage (After)` = (n() / nrow(data2)) * 100
      )
    
    # Merge summaries
    result <- full_join(summary_data1, summary_data2, by = variable) %>%
      mutate(
        `Percentage (Before)` = sprintf("%.2f%%", `Percentage (Before)`),
        `Percentage (After)` = sprintf("%.2f%%", `Percentage (After)`)
      )
    
    # Apply sorting
    if (sort == "asc") {
      result <- result %>% arrange(`Frequency (Before)`)
    } else if (sort == "desc") {
      result <- result %>% arrange(desc(`Frequency (Before)`))
    }
  } else {
    # For single dataset case
    result <- summary_data1 %>%
      mutate(
        `Percentage (Before)` = sprintf("%.2f%%", `Percentage (Before)`)
      ) %>%
      rename(`Frequency` = `Frequency (Before)`,
             `Percentage` = `Percentage (Before)`)
    
    # Apply sorting
    if (sort == "asc") {
      result <- result %>% arrange(Frequency)
    } else if (sort == "desc") {
      result <- result %>% arrange(desc(Frequency))
    }
  }
  
  # Rename the variable column
  result <- result %>%
    rename(!!capitalize_str(variable) := all_of(variable))
  
  # Display table
  result <- result %>%
    kbl() %>%
    kable_classic() %>%
    kable_styling(latex_options = c("striped"), full_width = F)
  
  # Return table
  return(result)
}

```

#### summary_table_nums()

A function was created to efficiently display the summary descriptives of a dataset and its given continuous variables.

```{r}
# Create function to create summary table for numerical variables
summary_table_nums <- function(data, variables) {
  # Compile summary table
  summary <- data.frame(
    Mean = sapply(variables, function(var) round(mean(data[[var]]), 3)),
    Min = sapply(variables, function(var) min(data[[var]])),
    Median = sapply(variables, function(var) median(data[[var]])),
    Max = sapply(variables, function(var) max(data[[var]])),
    SD = sapply(variables, function(var) round(sd(data[[var]]), 3))
  )
  
  # Temporarily rename rownames using capitalize_str
  rownames(summary) <- sapply(variables, capitalize_str)
  
  # Display summary table
  summary <- summary %>%
    kbl() %>%
    kable_classic(latex_options = c("striped"), full_width = TRUE)
  
  # Return table
  return(summary)
}
```

#### plot_histogram()

A function was created to easily plot the distribution of a given continuous variable in a histogram.

```{r}
# Function to create a histogram
plot_histogram <- function(data, variable, title = NULL, binwidth = NULL, bins = NULL) {
  
  # Compile plot
  plot <- ggplot(data, aes(x = .data[[variable]])) +
    geom_histogram(binwidth = binwidth, 
                   bins = bins, 
                   fill = "lightblue",
                   color = "black") +
    labs(
      title = title,
      x = capitalize_str(variable),
      y = "Frequency",
      fill = variable
    ) +
    theme_bw()
  
  # Return plot
  return(plot)
}
```

### Categorical variables

#### 1. gender

```{r}
# Verify structure of factors
str(data$gender)

# Verify levels of factors
levels(data$gender)
```

```{r}
# Show summary table
(v1_table <- summary_table_cat(data, 
                               variable = "gender")
)
```

##### Table of value frequency for gender

```{r}
# Show bar plot
(v1_plot <- ggplot(data) +
  geom_bar(aes(x = gender, fill = gender)) +
  scale_fill_manual(values = c("Male" = "lightblue", 
                               "Female" = "lightpink")) +
  labs(title = "Bar Plot of Gender Distribution",
       fill = "Gender",
       x = "Gender",
       y = "Frequency"
      ) +
  theme_bw()
)
```

#### 2. purchased

```{r}
# Verify structure of factors
str(data$purchased)

# Verify levels of factors
levels(data$purchased)
```

```{r}
# Show summary table
(v1_table <- summary_table_cat(data, 
                               variable = "purchased")
)
```

##### Table of value frequency for purchased

```{r}
# Show bar plot
(v1_plot <- ggplot(data) +
  geom_bar(aes(x = purchased, fill = purchased)) +
  scale_fill_manual(values = c("No" = "tomato", 
                               "Yes" = "lightgreen")) +
  labs(title = "Bar Plot of Purchased Distribution",
       fill = "Purchased",
       x = "Purchased",
       y = "Frequency"
      ) +
  theme_bw()
)
```

### Continuous variables

#### Table of summary descriptives for continuous variables

```{r}
summary_table_nums(data, c("age", 
                               "estimated_salary")
                   )
```

#### 3. age

```{r}
# Plot histogram
(v3_plot <- plot_histogram(data, 
                           "age", 
                           "Distribution of Age",
                            binwidth = 1)
)
```


#### 4. estimated_salary

```{r}
# Plot histogram
(v4_plot <- plot_histogram(data, 
                           "estimated_salary", 
                           "Distribution of Estimated Salary",
                            binwidth = 3000)
)
```

# Logistic Regression

## Functions

### compare_models()

A function was created to efficiently compare fit measures across logistic (or linear) models.

```{r}
# Function to compare fit measures of multiple models
compare_models <- function(data, response_var, compare_with_first = FALSE, model_type = "lm", ...) {
  # Capture models and their names
  models <- list(...)
  model_names <- as.character(substitute(list(...)))[-1] # Extract model names
  num_models <- length(models)
  
  # Initialize variables to store results
  aic_vals <- numeric(num_models)
  bic_vals <- numeric(num_models)
  mse_vals <- if (model_type == "lm") numeric(num_models) else NULL
  r_squared <- if (model_type == "lm") numeric(num_models) else NULL
  deviance_rss <- c(NA, rep(NA, num_models - 1))
  deviance_pvalue <- c(NA, rep(NA, num_models - 1))
  
  # Loop through models to compute fit measures
  for (i in seq_along(models)) {
    aic_vals[i] <- AIC(models[[i]])
    bic_vals[i] <- BIC(models[[i]])
    
    if (model_type == "lm") {
      mse_vals[i] <- mean(models[[i]]$residuals^2)
      r_squared[i] <- summary(models[[i]])$r.squared
    }
    
    # Perform ANOVA for model comparison
    if (i > 1) {
      if (compare_with_first) {
        base_model <- models[[1]]
      } else {
        base_model <- models[[i - 1]]
      }
      anova_test <- anova(base_model, models[[i]], test = ifelse(model_type == "glm", "Chisq", NULL))
      deviance_rss[i] <- if (model_type == "glm") anova_test$Deviance[2] else anova_test$RSS[2]
      deviance_pvalue[i] <- anova_test$`Pr(>Chi)`[2]
    }
  }
  
  # Compile results into a tibble
  fit_measures <- tibble(
    Model = capitalize_str(model_names),
    AIC = aic_vals,
    BIC = bic_vals
  )
  
  # Add R^2 and MSE for linear regression models
  if (model_type == "lm") {
    fit_measures <- fit_measures %>%
      mutate(
        MSE = mse_vals,
        `R^2` = round(r_squared, 3)
      )
  }
  
  # Add deviance for both model types
  fit_measures <- fit_measures %>%
  mutate(
    `Dev. RSS` = deviance_rss,
    `Dev. p-value` = deviance_pvalue
  )
  
  # Fancy kable formatting
  fit_measures <- fit_measures %>%
    kbl() %>%
    kable_classic(latex_options = c("striped"), full_width = TRUE)
  
  # Return fit measures
  return(fit_measures)
}


```

## Models

### Null model

```{r null-model}
# Define the null model
model_null <- glm(purchased ~ 1,
                  data = data,
                  family = binomial)
summary(model_null)
```
Null model intercept of -0.5862 converts to 35.75% purchasing probability. Found by converting the exponential of the log-odds to statistical odds.
As it is the null model, highly significant and strong prediction with outcome.

### Model 1: Gender as predictor

```{r model-1}
# Logistic regression with Gender as the sole predictor
model_1 <- glm(purchased ~ 
                 gender, 
               data = data, 
               family = binomial)
summary(model_1)
```
Here, Gender: 0 is assumed as "Female", the reference category. The intercept for that category is -0.5004. 37.50% purchasing probability

For GenderMale, the "Male" category, the coefficent is -0.1775. Being male reduces the log-odds of purchasing by 0.1775. Lower probability of purchasing compared to being female. The P-value is 0.395858, which is not statistically significant. Therefore Gender does not significantly affect the purchasing probability in this model.

Model Fit:

Null deviance is 512.7, the same as the null model. Residual deviance is slightly lower, but minimal.

The AIC is 1 point higher than the null model's AIC, which suggests it is a worse model in terms of fit + complexity. The slightly lower residual deviance does not outweigh the penalty for adding the extra parameter.

### Model 2: Gender + Age
```{r model-2}
# Logistic regression with Gender and Age as predictors
model_2 <- glm(purchased ~ 
                 gender +
                 age, 
               data = data,
               family = binomial)
summary(model_2)
```
Intercept of -8.11537, low probability of purchase, when age is 0. The z and p values determine the intercept to be highly statistically significant.

GenderMale indicates that being male increases the log-odds of purchasing by 0.09468 compared to females, holding Age constant. However, this effect is not statistically significant.

The coefficient for Age indicates that each one-unit increase in age increases the log-odds of purchasing by 0.18954, holding Gender constant. Older individuals are much more likely to purchase. For each additional year of age, the odds increase approximately by e^0.18954 â‰ˆ1.21 (a 21% increase in odds per year of age). Highly significant.

Much lower residual deviance (336.14) indicates better fit. Suggests age greatly improves model fit.

The AIC is much lower than both the null model (523.57) and Model 1 (524.85).

The addition of Age significantly improves the model's explanatory power while justifying the slight increase in complexity by 1 parameter.

### Model 3: Gender + Age + Estimated Salary
```{r model-3}
# Logistic regression with all predictors
model_3 <- glm(purchased ~ 
                 gender + 
                 age + 
                 estimated_salary, 
               data = data, 
               family = binomial)
summary(model_3)
```
Intercept is at -12.78, when all predictors are at baseline (Gender is Female, Age and EstimatedSalary are both 0). Highly statistically significant, according to the p and z values.

The coefficient for GenderMale suggests that being male increases the log-odds of purchase by 0.3338, while Age and EstimatedSalary remain constant. However, it is a small effect and not statistically significant (p-value: 0.274).

Each increase of age by 1 unit increases the log-odds of purchase by 0.2370, holding Gender and Estimated Salary constant. This corresponds to an increase of 27% in odds for each additional year of age. Age is highly statistically significant.

The coefficient for EstimatedSalary suggests that for every unit increase in salary, the log-odds of purchase increase by 0.00003644, while Gender and Age remain constant. This is statistically significant. This effect seems small, but do not forget that it applies to every single unit of currency.

The residual deviance has dropped significantly compared to all other models. Indicating that EstimatedSalary provides great improvement in model fit.

### Comparing Model Fit

```{r}
# Comparing model to null model
compare_models(data,
              "purchased",
              FALSE,
              "glm",
              model_null,
              model_1,
              model_2,
              model_3)
```

```{r model-comparison}
# Compare models using AIC
model_comparison <- tibble(
  Model = c("Null Model", "Model 1", "Model 2", "Model 3"),
  AIC = c(AIC(model_null), AIC(model_1), AIC(model_2), AIC(model_3))
)
model_comparison <- model_comparison %>%
  arrange(AIC)  # Sort models by AIC

# Display Model Comparison Table
print(model_comparison)
```

Model 3 is the best fit, with a significantly lower AIC value. The addition of Age and EstimatedSalary significantly improves the model fit.

## Assumptions

### Sufficient sample size

```{r}
# Show sample size
(sample_size <- nrow(data))
```

```{r}
# Calculate proportion of purchased
(purchased <- data %>%
  count(purchased) %>%
  mutate(prop = n / sum(n)))
```
```{r}
# Return smallest proportion of negative or positive cases.
(p <- min(purchased$prop))
```

```{r}
# Return number of predictors (k) of final model
(k <- length(coef(model_3)))
```

```{r}
# Calculate the minimum number of cases (n)
(n <- ceiling((10 * k) / p))
```

```{r}
# Check if sample size is sufficient
if (sample_size > n) {
  result <- "Sample size is sufficient"
} else {
  result <- "Sample size is not sufficient"
}

# Add values
result <- paste0(result, " (Samplze size = ", sample_size, ", minimum cases = ", n, ").")

# Print result
print(result)
```

### A2

### A3

## Conclusion
