---
title: "Assignment 2"
author: Judith van der Wolf (4661672) Jesse Nieuwkoop (1689959) Bas Bouwhuijzen (2130616)
  Jay de Jager (6990703)
date: "16-01-2025"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      expand: 3
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: '4'
subtitle: Fundamental Techniques in Data Science with R
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Introduction

For the second assignment we chose a new dataset that will hopefully give us better regression results. We are using a dataset called Customer Behavior from Kaggle. The data represents details about 400 clients of a company including the unique ID, the gender, the age of the customer, the salary, and the information regarding the buying decision - whether the customer decided to buy specific products or not. This dataset contains data on a new unknown product as part of a market assessment. We have a target variable `purchased` with values 0 or 1 (respectively no, and yes) indicating if the product was purchased by the customer. The other variables are `gender`, `age`, and `estimated_salary`. We will use these to predict whether the product was purchased. The `gender` variable has two values in our dataset: male and female. The `age` variable is a continuous numeric value. The `estimated_salary` is a value filled in by the customer with increments of 1000.

## Research Question

“What variables are the best predictors to whether the product was purchased or not?”

# Preperation
 
## Libraries

```{r}
library(tidyverse)    # For data manipulation
library(ggplot2)      # For plotting
library(ggpubr)       # For ggplot grid arrange
library(kableExtra)   # For fancy tables

library(lmtest)       # Assumption Checks
library(regclass)     # Assumption Checks
library(caret)        # Assumption Checks
```

## Read data

```{r}
# Read data
data <- read.csv("Customer_Behaviour.csv")

# View data structure
dplyr::glimpse(data)
```

# Preprocessing

## Rename

```{r}
# Rename column names to snake_case format
data <- data %>%
  dplyr::rename(
    user_id = User.ID,
    gender = Gender,
    age = Age,
    estimated_salary = EstimatedSalary,
    purchased = Purchased
  )
```

## Devide estimated_salary by 1000 for interpretation purposes 

```{r}
data <- data %>%
  mutate(
    estimated_salary = estimated_salary /1000
  )
```

## Duplicates

The data was checked for duplicates in user_id, because each used_id number should be unique in the dataset. Zero duplicates were found, thus no recards were removed.

```{r}
# Show count of duplicates
sum(duplicated(data$user_id))
```

## Missing values

The data was checked for missing values. No missing values were present.

```{r}
# Show count of NA values
data %>%
  dplyr::summarise(across(everything(), ~ sum(is.na(.)))) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "NA Count"
  )
```

## Convert to factor

The categorical variables (`gender` and `purchased`) were converted to factor data types.

```{r}
# Convert variables to factor types
data <- data %>%
  dplyr::mutate(
    gender = as.factor(gender),
    purchased = factor(purchased, levels = c(0, 1), labels = c("No", "Yes"))
  )
```

```{r}
# View new structure
str(data$gender)
str(data$purchased)
```

```{r}
# Show category distribution
table(data$gender)
table(data$purchased)
```

## Variable inspection

### Functions

#### capitalize_str()

A function was created to reformat object names to fancy string formatting when creating tables or plots.

```{r}
# Function to format column name to spaced, capitalized string
capitalize_str <- function(string) {
  # Replace underscores with spaces and capitalize each word
  formatted_name <- gsub("_", " ", string)
  formatted_name <- tools::toTitleCase(formatted_name)
  
  # Return result
  return(formatted_name)
}
```

#### summary_table_cat()

A function was created to efficiently display the value distribution of categorical variables.

```{r}
# Function to display comparison summary table
summary_table_cat <- function(data1, data2 = NULL, variable, sort = "none") {
  # Summarize data1
  summary_data1 <- data1 %>%
    group_by(across(all_of(variable))) %>%
    summarise(
      `Frequency (Before)` = n(),
      `Percentage (Before)` = (n() / nrow(data1)) * 100
    )
  
  if (!is.null(data2)) {
    # Summarize data2 if provided
    summary_data2 <- data2 %>%
      group_by(across(all_of(variable))) %>%
      summarise(
        `Frequency (After)` = n(),
        `Percentage (After)` = (n() / nrow(data2)) * 100
      )
    
    # Merge summaries
    result <- full_join(summary_data1, summary_data2, by = variable) %>%
      mutate(
        `Percentage (Before)` = sprintf("%.2f%%", `Percentage (Before)`),
        `Percentage (After)` = sprintf("%.2f%%", `Percentage (After)`)
      )
    
    # Apply sorting
    if (sort == "asc") {
      result <- result %>% arrange(`Frequency (Before)`)
    } else if (sort == "desc") {
      result <- result %>% arrange(desc(`Frequency (Before)`))
    }
  } else {
    # For single dataset case
    result <- summary_data1 %>%
      mutate(
        `Percentage (Before)` = sprintf("%.2f%%", `Percentage (Before)`)
      ) %>%
      rename(`Frequency` = `Frequency (Before)`,
             `Percentage` = `Percentage (Before)`)
    
    # Apply sorting
    if (sort == "asc") {
      result <- result %>% arrange(Frequency)
    } else if (sort == "desc") {
      result <- result %>% arrange(desc(Frequency))
    }
  }
  
  # Rename the variable column
  result <- result %>%
    rename(!!capitalize_str(variable) := all_of(variable))
  
  # Display table
  result <- result %>%
    kbl() %>%
    kable_classic() %>%
    kable_styling(latex_options = c("striped"), full_width = F)
  
  # Return table
  return(result)
}

```

#### summary_table_nums()

A function was created to efficiently display the summary descriptives of a dataset and its given continuous variables.

```{r}
# Create function to create summary table for numerical variables
summary_table_nums <- function(data, variables) {
  # Compile summary table
  summary <- data.frame(
    Mean = sapply(variables, function(var) round(mean(data[[var]]), 3)),
    Min = sapply(variables, function(var) min(data[[var]])),
    Median = sapply(variables, function(var) median(data[[var]])),
    Max = sapply(variables, function(var) max(data[[var]])),
    SD = sapply(variables, function(var) round(sd(data[[var]]), 3))
  )
  
  # Temporarily rename rownames using capitalize_str
  rownames(summary) <- sapply(variables, capitalize_str)
  
  # Display summary table
  summary <- summary %>%
    kbl() %>%
    kable_classic(latex_options = c("striped"), full_width = TRUE)
  
  # Return table
  return(summary)
}
```

#### plot_histogram()

A function was created to easily plot the distribution of a given continuous variable in a histogram.

```{r}
# Function to create a histogram
plot_histogram <- function(data, variable, title = NULL, binwidth = NULL, bins = NULL) {
  
  # Compile plot
  plot <- ggplot(data, aes(x = .data[[variable]])) +
    geom_histogram(binwidth = binwidth, 
                   bins = bins, 
                   fill = "lightblue",
                   color = "black") +
    labs(
      title = title,
      x = capitalize_str(variable),
      y = "Frequency",
      fill = variable
    ) +
    theme_bw()
  
  # Return plot
  return(plot)
}
```

### Categorical variables

#### 1. gender

```{r}
# Verify structure of factors
str(data$gender)

# Verify levels of factors
levels(data$gender)
```

```{r}
# Show summary table
(v1_table <- summary_table_cat(data, 
                               variable = "gender")
)
```

##### Table of value frequency for gender

```{r}
# Show bar plot
(v1_plot <- ggplot(data) +
  geom_bar(aes(x = gender, fill = gender)) +
  scale_fill_manual(values = c("Male" = "lightblue", 
                               "Female" = "lightpink")) +
  labs(title = "Bar Plot of Gender Distribution",
       fill = "Gender",
       x = "Gender",
       y = "Frequency"
      ) +
  theme_bw()
)
```

#### 2. purchased

```{r}
# Verify structure of factors
str(data$purchased)

# Verify levels of factors
levels(data$purchased)
```

```{r}
# Show summary table
(v2_table <- summary_table_cat(data, 
                               variable = "purchased")
)
```

##### Table of value frequency for purchased

```{r}
# Show bar plot
(v2_plot <- ggplot(data) +
  geom_bar(aes(x = purchased, fill = purchased)) +
  scale_fill_manual(values = c("No" = "tomato", 
                               "Yes" = "lightgreen")) +
  labs(title = "Bar Plot of Purchased Distribution",
       fill = "Purchased",
       x = "Purchased",
       y = "Frequency"
      ) +
  theme_bw()
)
```

### Continuous variables

#### Table of summary descriptives for continuous variables

```{r}
summary_table_nums(data, c("age", 
                               "estimated_salary")
                   )
```

#### 3. age

```{r}
# Plot histogram
(v3_plot <- plot_histogram(data, 
                           "age", 
                           "Distribution of Age",
                            binwidth = 1)
)
```

#### 4. estimated_salary

```{r}
# Plot histogram
(v4_plot <- plot_histogram(data, 
                           "estimated_salary", 
                           "Distribution of Estimated Salary",
                            binwidth = 3000)
)
```

### General description

After plotting all the predictor variables there were no unrealistic or unnatural values discovered. Furthermore, there were also no (theoretical) outliers present in the dataset. Since the dataset we are working with was flagged as 'clean', this was expected. This means that the dataset did not have to be modified in any way to be able to continue performing logistic regression.

# Logistic Regression

## Functions

### compare_models()

A function was created to efficiently compare fit measures across logistic (or linear) models.

```{r}
# Function to compare fit measures of multiple models
compare_models <- function(data, response_var, compare_with_first = FALSE, model_type = "lm", ...) {
  # Capture models and their names
  models <- list(...)
  model_names <- as.character(substitute(list(...)))[-1] # Extract model names
  num_models <- length(models)
  
  # Initialize variables to store results
  aic_vals <- numeric(num_models)
  bic_vals <- numeric(num_models)
  mse_vals <- if (model_type == "lm") numeric(num_models) else NULL
  r_squared <- if (model_type == "lm") numeric(num_models) else NULL
  deviance_rss <- c(NA, rep(NA, num_models - 1))
  deviance_pvalue <- c(NA, rep(NA, num_models - 1))
  
  # Loop through models to compute fit measures
  for (i in seq_along(models)) {
    aic_vals[i] <- AIC(models[[i]])
    bic_vals[i] <- BIC(models[[i]])
    
    if (model_type == "lm") {
      mse_vals[i] <- mean(models[[i]]$residuals^2)
      r_squared[i] <- summary(models[[i]])$r.squared
    }
    
    # Perform ANOVA for model comparison
    if (i > 1) {
      if (compare_with_first) {
        base_model <- models[[1]]
      } else {
        base_model <- models[[i - 1]]
      }
      anova_test <- anova(base_model, models[[i]], test = ifelse(model_type == "glm", "Chisq", NULL))
      deviance_rss[i] <- if (model_type == "glm") anova_test$Deviance[2] else anova_test$RSS[2]
      deviance_pvalue[i] <- anova_test$`Pr(>Chi)`[2]
    }
  }
  
  # Compile results into a tibble
  fit_measures <- tibble(
    Model = capitalize_str(model_names),
    AIC = aic_vals,
    BIC = bic_vals
  )
  
  # Add R^2 and MSE for linear regression models
  if (model_type == "lm") {
    fit_measures <- fit_measures %>%
      mutate(
        MSE = mse_vals,
        `R^2` = round(r_squared, 3)
      )
  }
  
  # Add deviance for both model types
  fit_measures <- fit_measures %>%
  mutate(
    `Dev. RSS` = deviance_rss,
    `Dev. p-value` = deviance_pvalue
  )
  
  # Fancy kable formatting
  fit_measures <- fit_measures %>%
    kbl() %>%
    kable_classic(latex_options = c("striped"), full_width = TRUE)
  
  # Return fit measures
  return(fit_measures)
}
```

## Models

*Note that for models 1_1, 1_2, and 1_3, the fit measures are compared to the null model. These fit measure comparisons can be found right after the interpretation of model 1_3 and before the interpretation of model 2.*

*However, the fit measures of model 2 are compared to the fit measures of model 1_2, and the fit measures of model 3 are compared to the fit measures of model 2. These fit measure comparisons can be found right after the interpretation of model 3.*

### Null Model

The log-odds for whether a product is purchased is -0.586 (intercept), and the odds are 0.556.

```{r}
# Define the null model
model_null <- glm(purchased ~ 1,
                  data = data,
                  family = binomial)

# Model summary and log-odds
summary(model_null)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_null))
```

### Model 1.1: gender

The reference category is "Female," with a log-odds of `purchased` of **-0.500** and corresponding odds of **0.606**. For men, the log-odds are **0.178** lower than for women, with odds of **0.837** (approximately **17% lower than women**). However, this difference is not statistically significant ($p = 0.396$), suggesting that `gender` is not a meaningful predictor of product purchase in this dataset.

The null deviance (**521.57**) and slightly lower residual deviance (**520.85**) indicate minimal improvement in fit, and the higher AIC (**524.85**) and BIC (**532.83**) further suggest that the null model is a better fit overall. The residual deviance for the gender model (**0.722**) is slightly lower than the null deviance, but this difference is not significant ($p = 0.395$), further supporting that adding gender does not meaningfully improve the model's fit. Thus, adding `gender` as a predictor does not meaningfully enhance the model's performance.

```{r}
# Logistic regression with Gender as the sole predictor
model_1.1 <- glm(purchased ~ 
                 gender, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_1.1)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_1.1))
```

### Model 1.2: age

The intercept (**-8.044**) represents the log-odds of product purchase for individuals aged 0, with corresponding odds close to 0 (**0.00032**), which is effectively negligible. For every additional year of age, the log-odds of product purchase increase by **0.189**, indicating that older individuals are more likely to purchase the product. The odds of purchase increase by  **1.208** for each additional year, meaning a 20.8% increase in the likelihood of purchase per year of age. This relationship is statistically significant ($p < 0.01$), suggesting that age is a strong predictor of product purchase.

The model fit comparison shows that adding age as a predictor (Model 1.2) significantly improves the model fit compared to the null model. Model 1.2 has a much lower AIC (**340.26**) and BIC (**348.24**) compared to the null model's AIC (**523.57**) and BIC (**527.56**). The residual deviance decreases substantially by **185.31**, and the difference is statistically significant ($p < 0.01$). These results indicate that age is a meaningful predictor, providing a significant improvement in model performance.

```{r}
# Logistic regression with Gender as the sole predictor
model_1.2 <- glm(purchased ~ 
                 age, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_1.2)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_1.2))
```

### Model 1.3: estimated_salary

The intercept (**-2.323**) represents the log-odds of product purchase for individuals with an estimated salary of 0, with corresponding odds of approximately **0.098**, indicating that the likelihood of purchase is very low at this salary level. For every one unit increase in estimated salary (1000 dollars), the log-odds of product purchase increase by **0.024**, suggesting that higher salaries are associated with a higher likelihood of product purchase. The odds of purchase increase by a factor of **1.024** for each additional unit, meaning a **2.4% increase in the likelihood of purchase per 1000 dollars increase of salary**, which is statistically significant ($p < 0.01$).

The model fit comparison shows that adding estimated salary as a predictor (Model 1.3) significantly improves the model fit compared to the null model. Model 1.3 has a lower AIC (**471.73**) and BIC (**479.71**) compared to the null model's AIC (**523.57**) and BIC (**527.56**). The residual deviance decreases by **53.85**, and the difference is statistically significant ($p < 0.01$). These results indicate that estimated salary is a meaningful predictor of product purchase, improving the model’s performance.

```{r}
# Logistic regression with Gender as the sole predictor
model_1.3 <- glm(purchased ~ 
                 estimated_salary, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_1.3)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_1.3))
```

##### Table of fit measure comparison (Model 1.1, 1.2, 1.3 & Null Model)

In the table below, model 1.1, 1.2, and 1.3 are compared to the fit of the null model.

```{r}
# Comparing model to null model
compare_models(data,
              "purchased",
              TRUE,
              "glm",
              model_null,
              model_1.1,
              model_1.2,
              model_1.3)
```

### Model 2: age + estimated_salary

The intercept (**-12.43**) represents the log-odds of product purchase when both age and estimated salary are held constant at 0, with corresponding odds of approximately **0**, indicating that the likelihood of purchase is extremely low at this baseline. When age increases by one year, the log-odds of product purchase increase by **0.234**, and the odds increase by a factor of **1.263**, meaning a **26.3% increase in the likelihood of purchase per year of age** while controlling for estimated salary. This relationship is statistically significant ($p < 0.01$).

When estimated salary increases by one unit, the log-odds of product purchase increase by **0.036**, and the odds increase by **1.037**, meaning a **3.7% increase in the likelihood of purchase per unit of salary** while controlling for age. This relationship is also statistically significant ($p < 0.01$).

The model fit comparison shows that adding both age and estimated salary as predictors (Model 2) significantly improves the model fit compared to Model 1.2 (which only includes age). Model 2 has a lower AIC (**283.05**) and BIC (**295.03**) compared to Model 1.2’s AIC (**340.26**) and BIC (**348.24**). The residual deviance decreases by **59.21**, and the difference is statistically significant ($p < 0.01$). These results indicate that the inclusion of estimated salary as a predictor further improves the model’s performance, suggesting that both age and estimated salary together are meaningful predictors of product purchase.

```{r}
# Logistic regression with Gender as the sole predictor
model_2 <- glm(purchased ~ 
                 age +
                 estimated_salary, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_2)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_2))
```

### Model 3: age * estimated_salary

Adding an interaction term between age and estimated salary reveals a significant interaction effect ($p < 0.01$), indicating that the relationship between estimated salary and the odds of product purchase depends on age, and vice versa. This suggests that the effect of estimated salary on product purchase is not constant across all ages and that the effect of age on product purchase varies with estimated salary. The odds of the interaction effect are close to 1, meaning that for every one-unit increase in both estimated salary and age, the odds of product purchase slightly decreases by approximately **0.00061%**. This slight decrease may be attributed to differences in the relationship between age, estimated salary, and product purchase behavior. For instance, younger individuals with higher incomes might spend more on products compared to older individuals with similar earnings. 

The model fit comparison shows that adding the interaction term (Model 3) significantly improves the model fit compared to Model 2 (which includes only main effects). Model 3 has a much lower AIC (**221.57**) and BIC (**237.54**) compared to Model 2’s AIC (**283.05**) and BIC (**295.03**). The residual deviance decreases by **63.48** compared to Model 2, and the difference is statistically significant ($p < 0.01$). This reduction in deviance indicates that the interaction term captures additional variability in product purchase behavior that is not explained by the main effects alone. These results highlight that the interaction between `age` and `estimated_salary` is a meaningful predictor, further improving the model's explanatory power. 

```{r}
model_3 <- glm(purchased ~ 
                 age * estimated_salary, 
               data = data, 
               family = binomial)

# Model summary and log-odds
summary(model_3)
cat("Log-odds of the coefficients: \n\n")
exp(coef(model_3))
```

##### Table of fit measure comparison (Model 1.2, 2 & 3)

In the table below, each model is compared to the fit measures of the model before. So model 2 is compared to model 1.2, and model 3 is compared to model 2.

```{r}
# Comparing model to null model
compare_models(data,
              "purchased",
              FALSE,
              "glm",
              model_1.2,
              model_2,
              model_3)
```

### Final Model

Model 3 will be selected as the final model because it demonstrates the best fit, with the lowest AIC and BIC among the compared models. Additionally, the significant reduction in residual deviance compared to Model 2 indicates that the interaction term meaningfully improves the model's explanatory power. This makes Model 3 the most appropriate choice for capturing the relationship between age, estimated salary, and product purchase.

## Assumptions

### A1: Linearity

```{r}
data$logit <- predict(model_3, type = "link")
```

As can be seen, the linearity of age to log odds is relatively  constant, Which means that age has a relatively consistent influence on the log odds of purchasing.

```{r}
data %>% 
ggplot(aes(age, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw()
```

As can be seen, the linearity of estimated_salary to log odds is relatively constant, Which means that salary has a relatively consistent influence on the log odds of purchasing.

```{r}
data %>% 
ggplot(aes(estimated_salary, logit))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(formula = 'y ~ x',
              method = "loess") + 
  theme_bw()
```

### A2: Predictor Matrix Rank

The VIF values in this model are quite high (exceeding the threshold of 10), indicating potential multicollinearity among the predictors. However, the high VIF values are expected because the model includes an interaction term (age * estimated_salary). Interaction terms are mathematically derived from the main effects, creating strong correlations and naturally leading to inflated VIF values. While this can make interpretation of the main effects challenging, it does not necessarily invalidate the model. The interaction term remains interpretable and may provide meaningful insights into how the predictors jointly influence the outcome.

```{r}
# Retrieve VIF values
vif_values <- car::vif(model_3, type = 'predictor')

# Force the output into a GVIF-like structure
gvif_table <- tibble(
  Variable = capitalize_str(names(vif_values)),
  VIF = vif_values,
  df = 1,
  GVIF = (vif_values^(1 / (1 + df)))
)

# Fancy kable table formatting
gvif_table %>%
  kbl() %>%
  kable_classic(latex_options = c("striped"), full_width = TRUE)
```

### A3: IID Binomial

#### Binary Outcome

The outcome is independently and identically binomially distributed. The bar plot shows there are two outcomes for purchased, so the outcome meets the outcome of a binomial distribution. 257 participants did not buy the product and 143 did. 

```{r}
data %>% 
  ggplot(aes(x = purchased, fill = purchased)) +
  geom_bar() +
  labs(x = "purchased",
       y = "Count",
       title = "Distribution of the outcome variable") +
  theme_bw()
```

```{r}
levels(data$purchased)
table(data$purchased)
```

#### Clustering

We didn't expect residual clustering, because the only variables in the dataset are whether or not a product is purchased, gender and people's estimated salary. There is no reason to believe that there were other variables that could have influenced the variation among the groups, at least no other variables that were measured. 

### Sufficient Sample Size

The minimum sample size is 112. The actual sample size is 400. This means that the assumption for sufficient sample size is met.

```{r}
# Show sample size
(sample_size <- nrow(data))
```

```{r}
# Calculate proportion of purchased
(purchased <- data %>%
  count(purchased) %>%
  mutate(prop = n / sum(n)))
```
```{r}
# Return smallest proportion of negative or positive cases.
(p <- min(purchased$prop))
```

```{r}
# Return number of predictors (k) of final model
(k <- length(coef(model_3)))
```

```{r}
# Calculate the minimum number of cases (n)
(n <- ceiling((10 * k) / p))
```

### Balanced Outcomes

The assumption of balanced outcomes was met.

```{r}
# Show distribution of outcomes
with(data, table(purchased) / length(purchased))
```

### Perfect Prediction

The model does not contain perfectly separable classes, therefore the assumption of perfect prediction is met.

```{r}
predicted_probs <- predict(model_3, type = "response")

perfect_predictions <- predicted_probs == 0 | predicted_probs == 1
any(perfect_predictions) 
```

### Influential Cases

As can be seen, there are no points that fall outside of the the dashed lines (Cook's distance), hence there are no outliers that are influential and therefore do not have to be treated.

```{r}
plot(model_3, which = c(4,5))
```

# Predicted Probabilities

```{r}
data <- data %>%
  mutate(piHat = predict(model_3, type = "response"),
    yHat = as.factor(ifelse(piHat <= 0.5, "No", "Yes"))
  )
```

```{r}
(conf <- confusionMatrix(data = data$yHat, reference = data$purchased))
conf$byClass["F1"]
```

1.    Overall Performance

    -   **Accuracy:** 0.8875 (88.75%)
        
        This means that 88.75% of the predictions made by the model are correct. An accuracy this high generally indicates good model performance. However, given that the "No" class is more prevalent (64.25%), accuracy alone is not sufficient to evaluate the model's effectiveness, given the non-perfect balance between "Yes" and "No" classes for `purchased`.
        
    -   **95% Confidence Interval (CI):** (0.8524, 0.9167)
        
        This range represents where the true accuracy is likely to fall 95% of the time. The narrow CI indicates a reliable estimate.
        
    -   **No Information Rate (NIR):** 0.6425
        
        This is the accuracy you would get by always predicting the majority class (in this case, "No"). Since our model's accuracy is much higher than the NIR, it is performing significantly better than a naive model.
        
    -   **P-Value for Accuracy > NIR:** <2e-16
        
        This very small p-value indicates that the improvement over the NIR is statistically significant.
        
2. Agreement

    -   Kappa: 0.7555
    
        Kappa accounts for chance agreement. A value of 0.75 indicates substantial agreement beyond chance, suggesting that the model makes accurate predictions consistently (Cohen, 1960).
        
3. Class-Specific Performance

    -   Sensitivity (Recall) for 'No': 0.9105 (91.05%)
    
        This indicates how well the model identifies the "No" class. In this case, the sensitivity is fairly high, which means most "No" cases are correctly classified.
        
    -   Specificity for 'Yes': 0.8462 (84.62%)
    
        Specificity reflects how well the model identifies the "Yes" class. This is also strong, but slightly lower than the sensitivity.
        
    -   Positive Predictive Value (Precision) for 'No': 0.9141 (91.41%)
    
        This represents how many of the predicted "No" cases are actually "No." The high precision indicates few false positives.
        
    -   Negative Predictive Value for 'Yes': 0.8403 (84.03%)
    
        This indicates the proportion of true "Yes" cases among all predicted "Yes." It’s lower than the precision but still acceptable.
        
    -   Balanced Accuracy: 0.8783
    
        This is the average of sensitivity and specificity, providing a balanced view of model performance for both classes. It is high, indicating overall good classification performance.
        
    -   F1 Score: 0.91 (91.23%)
    
        This indicates excellent model performance, balancing both precision and recall. This suggests the model is highly effective at correctly identifying the positive class ("No") while minimizing false positives and false negatives. Such a high F1 score demonstrates the model's reliability in classification tasks.
        
4. McNemar's Test P-Value: 1

    -   A p-value of 1 suggests there is no significant difference between the prediction errors for false positives and false negatives. The model treats these types of errors similarly (McNemar, 1947).

---

Strengths: High accuracy, sensitivity, and F1 score suggest excellent performance in identifying positive cases ("yes").
Weaknesses: Specificity is lower, meaning "no" predictions are less reliable, and McNemar's p-value suggests some bias in prediction errors.

---

## Conclusion



# References

-   Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46. https://doi.org/10.1177/001316446002000104
-   van Rijsbergen, C. J. (1979). Information Retrieval (2nd ed.). Butterworths. ISBN: 978-0408709291
-   McNemar, Q. (1947). Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2), 153–157. https://doi.org/10.1007/BF02295996
    
